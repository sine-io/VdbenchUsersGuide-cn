#### **1.31.1** ***\*summary.html\****

 

'summary.html' reports the total workload generated for each run per reporting interval, and the weighted average for all intervals *except* the first (used to be first and last, report examples have not been updated).

Note: the first interval will be ignored for the run totals unless the [warmup= parameter ](#_bookmark118)is used in which case you can ask Vdbench to ignore more than one interval.

```shell
Starting RD=rd1; I/O rate: 1000; elapsed=3; For loops: xfersize=1k

interval     i/o    MB/sec   bytes   read     resp     read    write     resp     resp queue  cpu% cpu% 
	  		rate   1024**2     i/o    pct     time     resp     resp      max   stddev depth sys+u  sys
	 1     833.00     0.81    1024   0.00    0.007    0.000    0.007    0.035    0.003   0.0  12.5  0.4
	 2     991.00     0.97    1024   0.00    0.007    0.000    0.007    0.032    0.004   0.0  31.6  0.0
	 3    1001.00     0.98    1024   0.00    0.007    0.000    0.007    0.086    0.004   0.0   7.4  0.6
avg_2-3    996.00     0.97    1024   0.00    0.007    0.000    0.007    0.086    0.004   0.0  19.4  0.3
```

 

|              |                                                              |
| ------------ | ------------------------------------------------------------ |
| interval     | Reporting interval sequence number. [See 'interval=nn' ](#_bookmark117)parameter. |
| I/O rate     | Average observed I/O rate per second.                        |
| MB sec       | Average number of megabytes of data transferred.             |
| bytes I/O    | Average data transfer size.                                  |
| read pct     | Average percentage of reads.                                 |
| resp time    | Average response time measured as the duration of the read/writerequest. All I/O times are in milliseconds. |
| read resp    | Average response time for reads                              |
| write resp   | Average response time for writes                             |
| resp max     | Maximum response time observed in this interval. The last line containstotal max. |
| resp stddev  | Standard deviation for response time.                        |
| queue depth  | Average I/O queue depth calculated by Vdbench. There may be slight differences with the Kstat results, this due to at what time during the I/O process the calculations are made.Realize also that Kstat reports on TWO queues: the host wait queue and the device active queue. |
| cpu% sys+usr | Processor busy = 100 -  (system + user time) (Solaris, Windows, Linux) |
| cpu% sys     | Processor utilization; system time.Note that Vdbench will display a warning if average cpu utilization during a test reaches 80%. This warns that you may not have enoughcpu cycles available to properly run at the highest workload possible. |



 

 

 

#### **1.31.2** ***\*totals.html; run\**** ***\*totals\****

 

The run totals report allows you to get a quick overview of all the totals without the need to scroll through page after page of detailed interval results.

 

```shell

Starting RD=rd1; I/O rate: 1000; elapsed=3; For loops: xfersize=1k
interval      i/o   MB/sec   bytes   read     resp     read    write     resp     resp  queue  cpu%  cpu%
			 rate  1024**2     i/o    pct     time     resp     resp      max   stddev  depth  sys+u  sys
avg_2-3     996.00     0.97    1024   0.00    0.007    0.000    0.007    0.086    0.004   0.0  19.4  0.3

Starting RD=rd1; I/O rate: 1000; elapsed=3; For loops: xfersize=2k

avg_2-3    1000.00     1.95    2048   0.00    0.007    0.000    0.007    0.054    0.004   0.0   7.6  0.2

Starting RD=rd1; I/O rate: 1000; elapsed=3; For loops: xfersize=3k

avg_2-3    1000.00     2.93    3072   0.00    0.007    0.000    0.007    0.056    0.004   0.0   9.2  0.1

```



#### **1.31.3** ***\*totals_optional.html: running\**** ***\*totals\****

 

This is an optional output file, created when using the 'report_run_totals=yes' parameter. This file is updated at the end of every reporting interval and reports the TOTAL amount of i/o done during the complete Vdbench **execution**, not just the current Run Definition.

This is only available for SD/WD workloads.

 

```shell
This report is created at the end of each successfully completed reporting interval.
Last reported interval is interval #5 on Mon Apr 11 09:42:57 MDT 2016
Overall execution totals logical i/o: Total iops:               159756.50
Total reads+writes:         1597565
Total gigabytes:               6.09
Total gigabytes read:          6.09
Total gigabytes write:         0.00
Total reads:                1597092
Total writes:                   473
Total readpct:                99.97
Total I/O or DV errors:            0 (.000000000000 errors per GB)
 
```

 

#### **1.31.4** ***\*summary.html for file system\**** ***\*testing\****

 

This sample report has been truncated. Three columns exist for each file system operation.

 

```shell

.Interval. .ReqstdOps.. ...cpu%... ....read.... ...write.... ..mb/sec... mb/sec .xfer. etc.etc
			rate resp  total sys  rate  resp   rate  resp  read write  total  size
      1    93.0  4.81  2.7  1.00   93.0  4.81   0.0  0.00  0.05  0.00   0.05   512
      2    98.0  2.16  5.3  1.27   98.0  2.16   0.0  0.00  0.05  0.00   0.05   512
      3   100.0  1.33  2.3  0.25  100.0  1.33   0.0  0.00  0.05  0.00   0.05   512
      4    99.0  0.58  2.2  0.75   99.0  0.58   0.0  0.00  0.05  0.00   0.05   512
      5    99.0  0.86  1.5  0.75   99.0  0.86   0.0  0.00  0.05  0.00   0.05   512

avg_2-5   99.0  1.23   2.8 0.75   99.0  1.23    0.0  0.00  0.05  0.00   0.05   512

```

 

|            |                                                              |
| ---------- | ------------------------------------------------------------ |
| Interval:  | Reporting interval sequence number. [See 'interval=nn' ](#_bookmark117)parameter. |
| ReqstdOps  | The total amount of *requested* operations. Though when asking for operation=read requires an open operation, this open has not been specifically requested and is therefore not included in this count. This open however ISreported in the ‘open’ column. For a format run this count includes all write operations. |
| cpu% total | Processor busy = 100 -  (system + user time) (Solaris, windows, Linux) |
| cpu% sys   | Processor utilization; system time                           |
| read       | Total reads and average response time in milliseconds.       |
| write      | Total writes and average response time in milliseconds.      |
| mb/sec     | Mb per second for reads, writes, and the sum of reads and writes. |
| xfer       | Average transfer size for read and write operations.         |
| …..        | Two columns each for all remaining operations.               |

  

Each operation, and also ReqstdOps have two columns: the amount of operations and the average response time. There are also columns for average xfersize and megabytes per second.

 

Note: due to the large amount of columns that are displayed here the precision of the displayed data may vary. For instance, a rate of 23.4 per second will be displayed using decimals, but a rate of 2345.6 will be displayed without decimals as 2345. I like my columns to line up J.

 

 

#### **1.31.5** ***\*logfile.html\****

'logfile.html' contains a copy of each line of information that has been written by the Java code to the terminal. Logfile.html is primarily used for debugging purposes. If ever you have a problem or a question about a Vdbench run, always add a tar or zip file of the complete Vdbench output directory in your email. Especially when crossing multiple time zones this can save a lot of time because usually the first thing I’ll ask for anyway is this tar or zip file. I can usually answer 99% of your questions when I have the output directory available.

 

 

#### **1.31.6** ***\*kstat.html\****

 

'kstat.html' contains Kstat statistics *for Solaris only*:

 

```shell

			 interval     KSTAT_i/o   resp    wait  service  MB/sec   read  busy avg_i/o avg_i/o   bytes   cpu%  cpu%
						     rate     time    time     time  1024**2  pct   pct  waiting  active  per_io  sys+usr sys
11:55:51.035 Starting RD=run1; I/O rate:   5000; Elapsed: 20 seconds. For loops: threads=8
11:56:00.023        1    4998.10     0.89     0.02     0.87    4.88 100.00  67.7    0.11    4.33    1024     5.9  2.4
11:56:04.087        2    5000.06     0.88     0.02     0.86    4.88 100.00  67.7    0.11    4.30    1024     2.2  0.5
11:56:08.024        3    5000.07     0.89     0.02     0.87    4.88 100.00  67.6    0.11    4.35    1024     1.4  0.2
11:56:12.013        4    4999.95     0.87     0.02     0.85    4.88 100.00  67.7    0.11    4.25    1024     1.4  0.4
11:56:16.013        5    4999.83     0.86     0.02     0.84    4.88 100.00  67.7    0.11    4.21    1024     1.3  0.4
11:56:16.101  avg_2-5    4999.98     0.88     0.02     0.86    4.88 100.00  67.7    0.11    4.28    1024     1.6  0.4

```

 

|                   |                                                              |
| ----------------- | ------------------------------------------------------------ |
| I/O rate:         | I/O rate per second over the duration of the reporting interval |
| resp time:        | Response time (the sum of wait time and service time). All I/Otimes are in milliseconds. |
| wait time:        | Average time each I/O spent queued on the host               |
| service time:     | Average time the I/O was being processed                     |
| MB/sec:           | Average data transfer rate per second                        |
| read pct:         | Average percentage of total I/O that was read                |
| busy pct:         | Average device busy percentage                               |
| avg #I/O waiting: | Average number of I/Os queued on the host                    |
| avg #I/O active:  | Average number of I/Os active                                |
| bytes per I/O:    | Average number of bytes transferred per I/O                  |
| cpu% sys+usr      | Processor busy = 100 -  (system + user time)                 |
| cpu% sys          | Processor utilization; system time                           |

 

Warning: Vdbench reports the sum of the service time and wait time correctly as ***response time\***.

*iostat* reports the same value as ***service time\***. The terminology used by *iostat* is ***wrong\***.



 

 

 

#### **1.31.7** ***\*histogram.html\****

This report shows the distribution of response times for both reads and writes combined, for reads, and for writes. When only reads or only writes are done there will of course be only one report. A histogram is generated for each SD and FSD and for each WD and FWD if there is more than one specified.

Note that this file can be directly read into Excel as a tab-delimited file.

 

```shell

Reads and writes:
min(ms) <     max(ms)        count   %%    cum%%  '+': Individual%; '+-': Cumulative%
0.000 <       0.020           91  25.8523  25.8523 ++++++++++++
0.020 <       0.040            2   0.5682  26.4205 ------------
0.040 <       0.060            0   0.0000  26.4205 -------------
0.060 <       0.080            1   0.2841  26.7045 -------------
0.080 <       0.100            0   0.0000  26.7045 -------------
0.100 <       0.200            0   0.0000  26.7045 -------------
0.200 <       0.400           50  14.2045  40.9091 +++++++-------------
0.400 <       0.600           40  11.3636  52.2727 +++++--------------------
0.600 <       0.800           14   3.9773  56.2500 +--------------------------
0.800 <       1.000            4   1.1364  57.3864 ----------------------------
1.000 <       2.000            7   1.9886  59.3750 ----------------------------
2.000 <       4.000           30   8.5227  67.8977 ++++-----------------------------
4.000 <       6.000           26   7.3864  75.2841 +++---------------------------------
6.000 <       8.000           15   4.2614  79.5455 ++-------------------------------------
8.000 <      10.000           31   8.8068  88.3523 ++++---------------------------------------
10.000 <     20.000           29   8.2386  96.5909 ++++--------------------------------------------
20.000 <     40.000           11   3.1250  99.7159 +------------------------------------------------
40.000 <     60.000            1   0.2841 100.0000 -------------------------------------------------
60.000 <     80.000            0   0.0000 100.0000 --------------------------------------------------
80.000 <    100.000            0   0.0000 100.0000 --------------------------------------------------
100.000 <   200.000            0   0.0000 100.0000 --------------------------------------------------
200.000 <   400.000            0   0.0000 100.0000 --------------------------------------------------
400.000 <   600.000            0   0.0000 100.0000 --------------------------------------------------
600.000 <   800.000            0   0.0000 100.0000 --------------------------------------------------
800.000 <  1000.000            0   0.0000 100.0000 --------------------------------------------------
1000.000 < 2000.000            0   0.0000 100.0000 --------------------------------------------------
2000.000 <      max            0   0.0000 100.0000 --------------------------------------------------

```



#### **1.31.8** ***\*nfs3/4.html\****

 

This report is created on Solaris if any of the workloads created use NFS mounted filesystems. This sample report has been truncated. One column exists for each NFS operation.

See also [Operations counts vs. nfsstat counts:](#_bookmark199)

 

```shell
			 interval getattr setattr lookup access read write commit create mkdir etc.etc. 
						 rate    rate   rate   rate  rate  rate   rate   rate rate
10:02:55.038        1     1.4     0.0    2.9    1.4   1.4   1.4    0.0    0.0  0.0
10:02:56.476        2     5.5     1.4   11.7    3.4   2.8   1.4    0.0    0.0  6.9
10:02:57.041        3     1.8     0.0    0.0    0.0   0.0   0.0    0.0    0.0  0.0
10:02:58.029        4     8.0     0.0   16.0    8.0   0.0   0.0    0.0    8.0  0.0
10:02:59.071        5    10.0     0.0   12.0   17.0   2.0   2.0    0.0    0.0  0.0
10:03:00.028        6    10.0     0.0    0.0    0.0   0.0   8.0    8.0    0.0  0.0
10:03:01.019        7     2.0     0.0    0.0    0.0   0.0   2.0    0.0    0.0  0.0

```



Note: due to the large amount of columns that are displayed here the precision of the displayed data may vary. For instance, a rate of 23.4 per second may be displayed using decimals, but a rate of 2345.6 will be displayed without decimals as 2345.



 

#### **1.31.9** ***\*flatfile.html\****

 

'flatfile.html' contains Vdbench generated information in a column by column ASCII format. The first line in the file contains a one word 'column header name'; the rest of the file contains data that belongs to each column. The objective of this file format is to allow easy transfer of information to a spreadsheet and therefore the creation of performance charts.

See ‘[Selective flatfile parsing’.](#_bookmark237)

 

This format has been chosen to allow backward compatibility with future changes. Specifically, by making data selection column header-dependent and not column number-dependent, we can assure that modifications to the column order will not cause problems with existing data selection programs.

On Solaris only, storage performance data extracted from Kstat is written to the flat file, along with CPU utilization information like user, kernel, wait, and idle times.

Flatfile.html data is written both for the original RAW I/O Vdbench functionality (SD/WD/RD) and for file system testing using FSD/FWD/RD parameters.



 

 

 

#### **1.31.10** ***\*skew.html\****

 

One of the many objectives of Vdbench is to allow users to run multiple concurrent workloads, with each workload getting a skew=nn percentage of the workload.

This report will give you information about how successful the skew has been, or better yet, that the requested skew has been met. There are a few scenarios where, because of contradicting user parameters, the skew percentage may not be reached, for instance a multi-jvm/slave run with one workload doing 100% sequential and an other workload doing random I/O. The sequential workload may run on only ONE slave, while the random workload may run on all available slaves. **Workload skew will ONLY work properly when all workloads are allowed to run on all slaves.**

Note that when using SD Concatenation Vdbench makes sure that this scenario can not happen, it is 'legacy' Vdbench workloads where this still may occur.

 

See also the ' abort_failed_skew=nn' parameter.

```shell

Vdbench output/skew.html Workload skew report

Skew information will only be generated if:
 - there is more than one Workload Definition (WD/FWD)
 - there is more than one Storage Definition (SD/FSD)
 - there is more than one Host
 - there is more than one Slave
(Though these rules may be ignored from time to time). 

Counts reported below are for non-warmup intervals.

				   i/o   MB/sec   bytes   read     resp     read    write    resp      resp queue
Slave:    		  rate  1024**2     i/o    pct     time     resp     resp     max    stddev depth
localhost-0   87601.40    85.55    1024 100.00    0.020    0.020    0.000    5.636    0.021  1.7
localhost-1   88191.20    86.12    1024 100.00    0.020    0.020    0.000    2.614    0.017  1.7
localhost-2   87267.20    85.22    1024 100.00    0.020    0.020    0.000    5.748    0.022  1.7
localhost-3   86301.60    84.28    1024 100.00    0.020    0.020    0.000    4.343    0.021  1.7
Total        349361.40   341.17    1024 100.00    0.020    0.020    0.000    5.748    0.020  7.0

Calculated versus requested workload skew. (Delta only shown if > 0.10% absolute) 
Note that for an Uncontrolled MAX run workload skew is irrelevant.

		     i/o   MB/sec   bytes   read     resp     read    write     resp    resp  queue       skew    skew     skew 
WD:         rate  1024**2     i/o    pct     time     resp     resp      max   stddev depth  requested observed   delta 
wd1    139905.20   136.63    1024 100.00    0.020    0.020    0.000    2.641    0.018   2.8     40.00%  40.05%
wd2    209456.20   204.55    1024 100.00    0.020    0.020    0.000    5.748    0.022   4.2     60.00%  59.95%
Total  349361.40   341.17    1024 100.00    0.020    0.020    0.000    5.748    0.020   7.0    100.00% 100.00%

```