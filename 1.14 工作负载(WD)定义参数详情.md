The Workload Definition parameters describe what kind of workload must be executed using the storage definitions entered. Note that a lot of these parameters can be overridden within a Run definition (RD) using ‘forxxx=’ parameters.



Example: wd=wd1,sd=(sd1,sd2),rdpct=100,xfersize=4k



| wd=default               | Sets defaults for all WDs that are entered later.            |
| ------------------------ | ------------------------------------------------------------ |
| wd=name                  | Unique [name for this Workload Definition ](#_bookmark97)(WD) |
| host=host_label          | Specify here which host you want this to run on.             |
| iorate=nn                | [Workload specific I/O ](#_bookmark107)rate                  |
| openflags=               | See [openflags=](#_bookmark92)                               |
| priority=                | [Workload specific I/O priority.](#_bookmark108)             |
| range=(nn,nn)            | [Limit seek range ](#_bookmark106)to a defined range within an SD. |
| rdpct=nn                 | [Read percentage.](#_bookmark100) Default 100.               |
| rhpct=nn                 | [Read hit percentage.](#_bookmark101) Default 0.             |
| sd=xx                    | Name(s) of [Storage Definition(s) ](#_bookmark99)to use      |
| seekpct=nn               | [Percentage of random seeks](#_bookmark104). Default seekpct=100 orseekpct=random. |
| skew=nn                  | [Percentage of skew ](#_bookmark103)that this workload receives from the totalI/O rate. |
| stride=(min,max)         | To allow for [skip-sequential I/O.](#_bookmark105)           |
| whpct=nn                 | [Write hit percentage. ](#_bookmark101)Default 0.            |
| xfersize=nn              | [Data transfer size.](#_bookmark102) Default 4k.             |
| xfersize=(nn,%%,…)       | Data transfer size distribution.                             |
| xfersize=(min,max,align) | Generate xfersize as a random value between min and max.     |

 

 

 

#### **1.13.1** ***\*'wd=name': Workload Definition\**** ***\*Name\****

 

'wd=name' uniquely identifies each Workload Definition. The WD name is used by the Run Definition parameters to identify which workloads to execute. When you specify ‘default’ as the WD name, the values entered will be used as default for all WD parameters that follow.

 

#### **1.13.2** ***\*‘host=host_label’\****

 

This parameter is only needed for multi-host runs where you do not want each workload to run on each host. For example:

wd=wd1,host=hosta,…. wd=wd2,host=hostb,…

 

 

#### **1.13.3** ***\*'sd=name': SD names used in\**** ***\*Workload\****

 

'sd=' selects a specific SD for this workload. A single SD name can be specified as 'sd=sd1', multiple SDs can be specified as either 'sd=(sd1,sd2,sd3,..)', a range as in ‘sd=(sd1-sd99)’, using a wildcard character 'sd=sd*', or a combination of these.

Note: When using an SD range, leading zeros are not allowed, e.g. (sd01-sd09). You may specify SDs also as an RD= subparameter.



 

 

 

#### **1.13.4** ***\*'rdpct=nn': Read\**** ***\*Percentage\****

 

'rdpct=' specifies the read percentage of the workload. rdpct=100 means 100% read; rdpct=0 means 100% write. rdpct=80 means a read/write ratio of 4:1, etc.

 

The default is 100% read. If there are no workloads for a specific SD with a read percentage other than 100 this SD is opened for input only.

 

This parameter may be overridden using the [‘forrdpct=’ ](#_bookmark127)parameter.

 

 

 

#### **1.13.5** ***\*'rhpct=nn' and 'whpct=nn': Read and Write Hit\**** ***\*Percentage\****

 

'rhpct=' and 'whpct=' specify the cache hit percentage that Vdbench will attempt to generate. This parameter is only useful when accessing raw devices or file systems mounted with 'forcedirectio' (or using ‘openflags’). For this to work, each volume is divided into two parts: the first one- megabyte of storage will be accessed for cache hits and is called the ***hit area\***. The remaining space on the SD will be accessed for cache misses and is called the ***miss area\***.

 

When Vdbench needs to generate a cache hit, it generates an I/O to the hit area, assuming that the data accessed is, or soon will be, residing in cache. Cache misses will be targeted toward the miss area, assuming that the miss area is large enough to ensure that most random accesses are cache misses. As you can see, this will only be useful when Vdbench has control over which physical volume and physical blocks will be ultimately read or written (so no file system cache).

 

The [**'hitarea=nn'** ](#_bookmark88)and [**'forhitarea='** ](#_bookmark131)parameters have been created to allow control over a volume's cache working set size. Some cached storage subsystems have different performance characteristics if too small a subset of the available cache is used. The total size of the SD hit or miss area must be at least 4 times the largest xfersize used, but Data Validation has much more stringent requirements, enough blocks to satisfy at least 2000 times the largest xfersize used.

 

These parameters may be overridden using the ['forrhpct=' ](#_bookmark128)or ‘[forwhpct=](#_bookmark129)’ parameters.

 

 

#### **1.13.6** ***\*'xfersize=nn': Data Transfer\**** ***\*Size\****

 

'xfersize=' specifies how much data is transferred for each I/O operation; allows (k)ilo and (m)ega bytes. This parameter may be overridden using the [‘forxfersize’ ](#_bookmark125)parameter.

 

Example: xfersize=4k (default)

 

You may also specify a distribution of data transfer sizes. Specify pairs of transfer size and percentages; the total of the percentages must add up to 100.

Example: xfersize=(4k,10,8k,10,16k,80)



 

 

A third option uses three values: xfersize=(min,max,align). This causes a random value between min and max, with a multiple of align to be generated. This also requires the use of the SD align= parameter. This parameter may not be used when dedupratio= is specified.

 

 

#### **1.13.7** ***\*'skew=nn': Percentage\**** ***\*skew\****

 

'skew=' specifies the percentage of the run's total I/O rate that will be generated for this workload. By default the total I/O rate will be evenly divided among all workloads. However, if the skew value is nonzero, a percentage of the requested I/O rate equal to the percentage skew value will be apportioned to one workload, with the remaining skew evenly divided among the workloads that have no skew percentage specified. The total skew for all workloads used in a Run Definition must equal 100%.

 

Example: Five workload definitions, one workload specifies 'skew=60'. This workload receives 60 percent of the requested I/O activity while the other four workloads each receive 10% of the requested I/O rate, with a total of 100%.

 

Vdbench generates I/O workloads by sending new I/O requests to a volume's (SD) internal work queue. This work queue has a maximum queue depth of 2000 per SD. If an SD cannot keep up with its requested workload and the queue fills up, Vdbench will not be able to generate new I/O requests for this and all other SDs until space in the queue becomes available again. This means that if you send 1000 IOPS to an SD that can handle only 100 IOPS, and 50 IOPS to a similar device, the queue for the first device will fill up, and I/O request generation for the second device will be held up. This has been done to enable Vdbench to preserve the requested workload skew while still allowing for a temporary 'backlog' of requested I/Os.

 

Note: see also the ' abort_failed_skew=nn' parameter.

 

To accommodate users that still would like to run an ***uncontrolled\*** workload see ['iorate=max'](#_bookmark113).

 

 

 

#### **1.13.8** ***\*'seekpct=nn': Percentage of Random\**** ***\*Seeks\****

 

'seekpct=' specifies how often a seek to a random lba will be generated. See also the stride= parameter for skip sequential processing.

 

| seekpct=100 orseekpct=random   | Every I/O will go to a different random seek address.        |
| ------------------------------ | ------------------------------------------------------------ |
| seekpct=0 orseekpct=sequential | There will be NO random seeks, and the run will therefore be purely sequential. When the end of the volume or file is reached, Vdbench will continue at the beginning, unless 'seekpct=eof' (seebelow) is specified. Be aware that if the volume size is smaller than |



 

|                                     | the cache size, continued processing will be all cache hits. Note: a 100% sequential workload is allowed to run on only ONE JVM, because without that limitation we could end up reading blocks 1,1,1,2,2,2,3,3,3,etc. Wonderful marketing performance numbers, but we're trying to stay honest here. |
| ----------------------------------- | ------------------------------------------------------------ |
| seekpct=seqnz seekpct=sequentialnz  | The ‘**nz**’ here tells Vdbench to NOT start on block zero each time, but instead start on a random block within the SD.This lowers the possibility that a second Vdbench run will just find all the data in cache. |
| seekpct=20                          | On average, 20% of the I/O operations will start at a new random seek address. This means that on average there will be one random seek, with 5 consecutive blocks of data transferred.See stride= below for skip sequential I/O. |
| seekpct=-1 orseekpct=eof            | A negative *one* value causes a sequential workload to terminate as soon as end-of-file is reached. Vdbench will continue until allworkloads using seekpct=eof have reached EOF. |
| seekpct=poissonseekpct=(poisson,nn) | A Poisson seek distribution. See [fileselect=(poisson,nn)](#_bookmark180) |

 

The randomizer used to generate a seek address is seeded using the host's time of day in micro seconds multiplied by the relative position of the Storage Definition (SD) defined for the workload (WD). For sequential processing, I/O for raw devices always starts at the ***second\*** block as defined by the data transfer size (block zero is never used).

 

This parameter may be overridden using the [‘forseekpct=’ ](#_bookmark130)parameter, though forseek will only accept numeric values, not 'random' or 'seq'.

 

#### **1.13.9** ***\*stride=(min,max): Skip-sequential\**** ***\*I/O.\****

 

The stride= parameter changes the behavior when a new lba has to be generated because of the use of the seekpct= parameter. Instead of generating a brand new random lba, Vdbench skips 'n' bytes, 'n' being a random value between min and max, using a multiple of the current selected data transfer size. When end of volume is reached we start again at the beginning.

 

#### **1.13.10** ***\*'range=nn': Limit Seek\**** ***\*Range\****

 

By default, the whole SD will be used. To limit the seek range for a workload, specify the starting and ending range of the SD: 'range=(40,60)' will limit I/O activity starting at 40% into the SD and ending at 60% into the SD.

If the max value is larger than 100 but smaller than 200, Vdbench will consider this a wrap across the end of your volume. For instance with range=(90,110) , Vdbench will generate an I/O workload using the last 10% and the first 10% of your volume.



 

When the values are greater than 200, the values will be considered given in *bytes* instead of in

*percentages*; e.g., ‘range=(1g,2g)’. Also see [Hot Bands.](#_bookmark135)

 

 

#### **1.13.11** ***\*‘iorate=’ Workload specific I/O\**** ***\*rate.\****

 

Normally the I/O rate for a workload is controlled by the rd=xxx,iorate= parameter, together with the workload skew= parameter. With the workload specific iorate= parameter you can now give a FIXED I/O rate to a workload, while the other workloads continue to be controlled by the rd=xxx,iorate= and the workload skew parameters.

When used, ‘priority=’ must also be specified..

 

This option was initially created to test a ‘what if’: “If I run a Video On Demand (VOD) workload , what will the impact on performance be if I add some maintenance or video editing workload?”.

 

#### **1.13.12** ***\*‘priority=’ Workload specific I/O\**** ***\*priority.\****

 

All I/O in Vdbench is scheduled using the expected I/O arrival times, which is obtained from the requested I/O rate. For instance, iorate=100 starts a new I/O on average every 1000/100 = 10 milliseconds. There are no I/O priorities within devices or workloads.

The new ‘priority=’ parameter attempts to change this.

Normally I/O in Vdbench is handled using internal ‘work’ fifo queues. When the priority= parameter is used, any work in a higher priority fifo queue is processed before any lower priority fifo is checked causing I/O  for that higher priority workload to be started first.

When any priority is specified, ALL workloads will have to have a priority specified. Priorities go from 1 (highest) to ‘n’ (lowest), and must be in sequence (priorities must be for instance 1 and 2, not 1 and 3).

Warning though: allowing for a specific workload iorate and priority does not guarantee that if you ask for 100 IOPS and your system can do only 50 that Vdbench magically gets the workload done.