Vdbench 504 introduces two main new pieces of functionality for raw I/O: "hot band" or "hotband" workloads and SD concatenation.

 [SNIA - 热带](https://www.snia.org/education/online-dictionary/term/hot-band)：访问频率相对较高的存储地址范围。

Note: Hot banding does NOT require SD concatenation, and SD concatenation does NOT require hot banding though it is highly *recommended* to use hot banding when using SD concatenation.

 

| Hotband parameter, e.g. wd=wd1,sd=*,hotband=(10,20) |                                                              |
| --------------------------------------------------- | ------------------------------------------------------------ |
| hotband=(start,end)                                 | Specify the starting and ending point of a hotband. Can be specified in either percentages 'hotband=(10,20)', or in bytes,e.g. 'hotband=(30g,60g)' |

  

Hot Band workloads: Vdbench until now has basically had three types of workloads when doing raw I/O: pure sequential, pure random, and a mix of random and sequential. Sequential speaks for itself, but it is the 'random' that started becoming a problem. Random for I/O has always meant that for each I/O a new random seek address ***over the whole volume\*** (SD) was generated. We all know of course, that it is highly unlikely that there are any real workloads that access data in a pure random fashion. Typically there is some locality of reference going on, where a big portion of the I/O is done against a smaller subset of the amount of storage that is available.

Of course, doing only pure random I/O against all available storage also does not make for a very nice test for storage devices that have one or more levels of cache.

 

To resolve this problem a new type of random workload has been created, a workload called 'hot-banding'. The new “hot band” I/O workload provides a workload that considers the contribution of read caching. This workload allows skewed access across one or more subsets of the available storage. This skewed access tends to hold data in cache and creates "cache hits" for improved throughput and performance.

 

Introducing this new hot banding workload created a different problem though: until now, any workload given to Vdbench was executed identically on each single volume given to Vdbench. With an objective to create hot bands forcing identical hot bands to every single volume of course is a huge contradiction.

 

This in turn required the second large change to Vdbench, something that we call "SD concatenation", with SD of course being a Vdbench Storage Definition, any volume/lun/disk/drive/slice/partition/file that you tell Vdbench to use.

SD concatenation basically is a simple volume manager, where all volumes (SDs) given to a workload are treated as if they are one large concatenated storage device. The hot band workloads then are executed against these concatenated volumes.

 

This of course was not all: normally, by using the 'threads=' parameter you tell Vdbench what the maximum amount of concurrent outstanding I/O may be against each SD. With hot band workloads dedicating threads to an SD that possibly is not or rarely used of course does not make



 

sense, so, when defining hot bands, the amount of threads to be used will be given to the concatenated SD instead. Be careful though, the default thread count is still just eight, so make sure you give the workloads what they need.

 

How many threads do you need? If you are running just one single workload it may not be too difficult to figure that out, but if you run a dozen different workloads concurrently things can get ugly fast. So for that, you can, for SD concatenation only, specify a thread count for a Run Definition (RD), and all threads will be shared between all SDs, and all workloads. Note though that you may end up having 500 threads and 500 volumes, but all 500 threads will be used for the slowest volume.

 

In normal Vdbench operation the Run Definition 'threads=' or 'forthreads=' parameter overrides the threads= value for each individual SD. With SD concatenation active however this is the TOTAL amount of threads that will be shared.

 

SD concatenation and sequential I/O: Of course, no good deed goes unpunished. When you have twelve drives and treat them as one large concatenation, and are doing sequential I/O on that concatenated SD, you get only ONE active drive. That of course then results in slow (one drive only) throughput. So for this, either don't use concatenation, or ask Vdbench to start doing [independent sequential streams ](#_bookmark94)using the streams= parameter.

 

New or changed parameters:

· concatenate=yes	Must be specified BEFORE your first SD parameter. Note that you can not have a mix of concatenated and not concatenated SDs.

· wd=xxx,sd=(sd1,sd2,….)	With SD concatenation active all these SDs will be treated as ONE large concatenated SD.

· wd=xxx,sd=(…..),……,threads=nn This specifies the total amount of threads to be shared by this workload. Can only be used with SD concatenation.

· rd=xxx,……,threads=nn	With SD concatenation active this is the amount of threads shared between all SDs and workloads.

· sd=xxx,……,streams=nn	This parameter has been moved and now is a Workload Definition (WD) parameter, e.g. wd=xxx,….,streams=nn.

· sd=xxx,…,streams=	This parameter has been moved to the Workload Definition and had its meaning slightly changed.