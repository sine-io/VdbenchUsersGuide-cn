Vdbench 504 introduces two main new pieces of functionality for raw I/O: "hot band" or "hotband" workloads and SD concatenation.

> 译者注： [SNIA - 热带](https://www.snia.org/education/online-dictionary/term/hot-band)：访问频率相对较高的存储地址范围。
>

Note: Hot banding does NOT require SD concatenation, and SD concatenation does NOT require hot banding though it is highly *recommended* to use hot banding when using SD concatenation.

 

Vdbench 504引入了两个主要的新功能用于原始I/O：热带（hot band/hotband）工作负载以及SD串联。

需要注意的是，热带不需要SD串联，而SD串联也不需要热带，尽管在使用SD串联时强烈建议使用热带。



| 参数                                                  | 说明                                                         |
| ----------------------------------------------------- | ------------------------------------------------------------ |
| Hotband parameter, e.g. `wd=wd1,sd=*,hotband=(10,20)` |                                                              |
| hotband=(start,end)                                   | Specify the starting and ending point of a hotband. Can be specified in either percentages 'hotband=(10,20)', or in bytes,e.g. 'hotband=(30g,60g)' |

Hot Band workloads: Vdbench until now has basically had three types of workloads when doing raw I/O: pure sequential, pure random, and a mix of random and sequential. Sequential speaks for itself, but it is the `random` that started becoming a problem. Random for I/O has always meant that for each I/O a new random seek address **over the whole volume** (SD) was generated. We all know of course, that it is highly unlikely that there are any real workloads that access data in a pure random fashion. Typically there is some locality of reference going on, where a big portion of the I/O is done against a smaller subset of the amount of storage that is available.

Of course, doing only pure random I/O against all available storage also does not make for a very nice test for storage devices that have one or more levels of cache.

To resolve this problem a new type of random workload has been created, a workload called 'hot-banding'. The new “hot band” I/O workload provides a workload that considers the contribution of read caching. This workload allows skewed access across one or more subsets of the available storage. This skewed access tends to hold data in cache and creates "cache hits" for improved throughput and performance.

Introducing this new hot banding workload created a different problem though: until now, any workload given to Vdbench was executed identically on each single volume given to Vdbench. With an objective to create hot bands forcing identical hot bands to every single volume of course is a huge contradiction.

This in turn required the second large change to Vdbench, something that we call "SD concatenation", with SD of course being a Vdbench Storage Definition, any `volume/lun/disk/drive/slice/partition/file` that you tell Vdbench to use.

SD concatenation basically is a simple volume manager, where all volumes (SDs) given to a workload are treated as if they are one large concatenated storage device. The hot band workloads then are executed against these concatenated volumes.

This of course was not all: normally, by using the `threads=` parameter you tell Vdbench what the maximum amount of concurrent outstanding I/O may be against each SD. With hot band workloads dedicating threads to an SD that possibly is not or rarely used of course does not make sense, so, when defining hot bands, the amount of threads to be used will be given to the concatenated SD instead. Be careful though, the default thread count is still just eight, so make sure you give the workloads what they need.

How many threads do you need? If you are running just one single workload it may not be too difficult to figure that out, but if you run a dozen different workloads concurrently things can get ugly fast. So for that, you can, for SD concatenation only, specify a thread count for a Run Definition (RD), and all threads will be shared between all SDs, and all workloads. Note though that you may end up having 500 threads and 500 volumes, but all 500 threads will be used for the slowest volume.

In normal Vdbench operation the Run Definition `threads=` or `forthreads=` parameter overrides the `threads=` value for each individual SD. With SD concatenation active however this is the TOTAL amount of threads that will be shared.

SD concatenation and sequential I/O: Of course, no good deed goes unpunished. When you have twelve drives and treat them as one large concatenation, and are doing sequential I/O on that concatenated SD, you get only ONE active drive. That of course then results in slow (one drive only) throughput. So for this, either don't use concatenation, or ask Vdbench to start doing [independent sequential streams ](#_bookmark94)using the `streams=` parameter.

New or changed parameters:

- `concatenate=yes`	Must be specified BEFORE your first SD parameter. Note that you can not have a mix of concatenated and not concatenated SDs.

- `wd=xxx,sd=(sd1,sd2,….)`	With SD concatenation active all these SDs will be treated as ONE large concatenated SD.

- `wd=xxx,sd=(…..),……,threads=nn` This specifies the total amount of threads to be shared by this workload. Can only be used with SD concatenation.

- `rd=xxx,……,threads=nn`	With SD concatenation active this is the amount of threads shared between all SDs and workloads.

- `sd=xxx,……,streams=nn`	This parameter has been moved and now is a Workload Definition (WD) parameter, e.g. `wd=xxx,….,streams=nn`.

- `sd=xxx,…,streams=`	This parameter has been moved to the Workload Definition and had its meaning slightly changed.



热带工作负载：直到现在，Vdbench在进行原始I/O时基本上有三种工作负载类型：纯顺序、纯随机和随机和顺序的混合。顺序工作负载不言自明，但是“随机”开始成为一个问题。对于I/O来说，随机始终意味着**在整个卷（SD）上**为每个I/O生成一个新的随机寻址地址。我们当然都知道，几乎没有真正的工作负载是以纯随机方式访问数据的。通常情况下，存在某种引用局部性，即对可用存储量的较小子集进行了大部分I/O。

当然，仅对所有可用存储执行纯随机I/O也不适合对具有一个或多个缓存级别的存储设备进行测试。

为了解决这个问题，创建了一种新类型的随机工作负载，称为“热带”工作负载。新的“热带”I/O工作负载提供了一种考虑读取缓存贡献的工作负载。该工作负载允许在一个或多个可用存储的子集之间进行倾斜访问。这种倾斜访问倾向于将数据保留在缓存中，并创建“缓存命中”以提高吞吐量和性能。

然而，引入这种新的热带工作负载产生了一个不同的问题：直到现在，Vdbench所提供的任何工作负载都在每个单独的卷上执行相同的操作。但是，为了创建热带，强制对每个单独的卷使用相同的热带显然是一个巨大的矛盾。

因此，为了解决这个问题，需要对Vdbench进行第二次重大改变，我们称之为“SD串联”，SD当然是Vdbench的存储定义，即您告诉Vdbench要使用的任何卷/逻辑单元/磁盘/驱动器/切片/分区/文件。

SD串联基本上是一个简单的卷管理器，其中所有卷（SD）提供给一个工作负载的情况下，会被视为一个大的串联存储设备。然后，热带工作负载将针对这些串联卷执行。

当然，这还不是全部：通常情况下，通过使用`threads=`参数，您告诉Vdbench每个SD可以有多少并发未完成的I/O。但是，将线程分配给可能根本不被使用或很少被使用的SD当然是没有意义的。因此，在定义热带时，将使用的线程数量将指定给串联的SD。但是请注意，缺省的线程数仍然只有八个，所以确保您为工作负载提供所需的线程。

需要多少线程呢？如果您只运行一个工作负载，那可能不太难弄清楚，但是如果您同时运行了十几个不同的工作负载，情况可能会很快变得混乱。因此，对于仅SD串联，您可以为运行定义（RD）指定一个线程计数，所有线程将在所有SD和所有工作负载之间共享。但是请注意，您可能最终会拥有500个线程和500个卷，但所有500个线程将用于最慢的卷。

在正常的Vdbench操作中，运行定义的`threads=`或`forthreads=`参数将覆盖每个单独SD的 `threads=` 值。然而，在激活SD串联时，这是将要共享的线程的总数。

SD串联和顺序I/O：当然，善行无迹。当您拥有十二个驱动器并将它们视为一个大的串联时，并且在该串联SD上进行顺序I/O时，您只会得到一个活动驱动器。这当然会导致吞吐量慢（只有一个驱动器）。所以为了解决这个问题，要么不使用串联，要么要求Vdbench开始使用[independent sequential streams](#_bookmark94)参数使用`streams=`参数。

新的或更改的参数：

- `concatenate=yes` 必须在您的第一个SD参数之前指定。请注意，您不能混合使用串联和非串联的SD。
- `wd=xxx,sd=(sd1,sd2,….)` 在SD串联激活时，所有这些SD将被视为一个大的串联SD。
- `wd=xxx,sd=(…..),……,threads=nn` 指定此工作负载共享的总线程数。只能与SD串联一起使用。
- `rd=xxx,……,threads=nn` 在SD串联激活时，这是所有SD和工作负载之间共享的线程数量。
- `sd=xxx,……,streams=nn` 此参数已被移动，现在是一个工作负载定义（WD）参数，例如 `wd=xxx,….,streams=nn`。
- `sd=xxx,…,streams=` 这个参数已经被移到工作负载定义中，并且其含义略有改变。