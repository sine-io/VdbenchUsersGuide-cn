#### **1.13.1** ***\*'sd=name': Storage Definition\**** ***\*Name\****

 

'sd=' uniquely identifies each Storage Definition. The SD name is used by the Workload Definition (WD) and Run Definition (RD) parameters to identify which SDs to use for its workload.

When you specify ‘default’ as the SD name, the values entered will be used as default for all SD parameters that follow.

The ‘name’ can contain any free-format name, special characters not allowed.

 

#### **1.13.2** ***\*'lun=lun_name': LUN or File\**** ***\*Name\****

 

'lun=' describes the name of the raw disk or the file name of the file system file to use. Be careful that you do not specify any disk that contains data that you do not want to lose. Been there, done that J. This is the main reason why Vdbench does not require root access. You don’t want to lose your root disk.

*Please don’t think, “I’ll only be* *reading* *the customer’s currently active production disks”. One accidental rdpct= with a value different than 100 and all data will be gone. That has happened at least twice!*

 

| lun=/dev/rdsk/cxt0d0s0    | Name of raw Solaris disk to be used for this SD.             |
| ------------------------- | ------------------------------------------------------------ |
| lun=/dev/vx/rdsk/cxt0d0s0 | Name of a raw VXVM volume                                    |
| lun=/home/dir/filename    | Solaris file system filename. No control over physical I/Oguaranteed since the file system may use system cache. |
| lun=\\.\d:                | Name of raw mounted Windows disk to be used for this SD.     |
| lun=\\.\PhysicalDrive1    | Physical number of raw Windows disk to be used for this SD.  |
| lun=c:\temp\filename      | Windows: file system file name.                              |
| lun=\\.\c:\temp\filename  | Windows: raw access to file.                                 |

 

The raw disk or the file system file will be opened for input only unless 'rdpct= is specified with any value other than 100 (default). This allows Vdbench to execute with read-only access to disks or files.

Block zero on a raw volume will never be accessed, this to prevent the volume label from being overwritten. This is accomplished by never allowing Vdbench to generate a seek address of zero, no matter how large the data transfer size is. This also implies that block zero will never be read.

 

Note: some volume managers will return a NULL block each time that a block is read that has never been written. Though of course this gives wonderful performance numbers the disk is never read and this therefore is NOT a valid workload. When you have a storage device like this, make sure that you first pre-format the whole volume using:



 

sd=sd1,lun=xx wd=wd1,sd=*,xf=1m,seekpct=eof,rdpct=0 rd=rd1,wd=*,iorate=max,elapsed=100h,interval=60

(The elapsed time must be large enough to completely format the volume. Vdbench will stop when done, but if the elapsed time is too short Vdbench may terminate too early before the volume has been completely formatted).

 

You may also specify formatsds=yes as a General Parameter, but remember, this will cause the format to be done each time the same parameter file is used.

 

Note: When an SD is recognized to be a raw device (lun name starts with "\\" on Windows or "/dev" on Unix), Vdbench will refuse to write to block zero, this to avoid overwriting a volume label.

 

 

 

#### **1.13.3** ***\*‘host=name’\****

 

This parameter is only needed when you do a multi-host run where the lun names on each host are different. For instance if a lun is /dev/rdsk/a on hosta but it is named /dev/rdsk/b on hostb then you’ll have to tell Vdbench about it.

 

The ‘lun’ and ‘host’ parameters in this case have to be entered in pairs, connecting a lun name to a host name, e.g.:

sd=sd1,lun=/dev/rdsk/a,host=hosta,lun=/dev/rdsk/b,host=hostb

 

By default Vdbench assumes that the lun names on each host are identical.

 

Note: when using SD concatenation on multiple hosts Swat will do an extra verification to make sure you have specified the proper device names, and will even correct your definition if possible. Example:

 

*sd=sd1,host=systemA,lun=/dev/rdsk/1,host=systemB,lun=/dev/rdsk/2 sd=sd2,host=systemA,lun=/dev/rdsk/2,host=systemB,lun=/dev/rdsk/1*

 

Though technically it is of course (sadly) possible that this is correct, it likely is not.

Vdbench, by writing 'markers' to these disks on the Vdbench master and then verifying them on the clients will verify that this is incorrect, and will internally change this to:

 

*sd=sd1,host=systemA,lun=/dev/rdsk/1,host=systemB,lun=/dev/rdsk/1 sd=sd2,host=systemA,lun=/dev/rdsk/2,host=systemB,lun=/dev/rdsk/2*

 

(If the disk names are identical on all sides, then the extra 'host=' parameter is not needed)

 

#### **1.13.4** ***\*‘count=(nn,mm)’\****



 

This parameter allows you to quickly create a sequence of SDs, e.g. sd=sd,lun=/dir/file,count=(0,8) results in sd0-sd7 for /dir/file0-7.

 

You may also specify a 'printf' mask, e.g.

sd=sd,lun=/dir/file%04d,count=(0,2) This will result in sd1+2 for /dir/file0001 and file0002.

 

 

Note: I have been asked numerous times to also support something like lun=/dev/rdsk/c0*.

I consider this far too dangerous though and therefore decided against it. Once upon a time about 40 years ago I accidentally erased all disk drives on a production system (*05:00 am*). You don't ever want to do thatJ.

 

#### **1.13.5** ***\*'size=nn: Size of LUN or\**** ***\*File\****

 

'size=' describes the size of the raw disk or file. You can enter this in bytes, kilobytes, megabytes, gigabytes or terabytes (k/m/g/t). If not specified, the size will be taken from the raw disk or from the file. Vdbench supports addresses larger than 2GB.

If this is a non-existing file, or an existing file that is not large enough, a separate Vdbench Run Definition named ‘File_format_or_append_for_sd=’ is automatically inserted and executed that will do a sequential write or append to the file until the file is full. This replaces the need to create a new file using *mkfile* or other utility.

Use 'formatsds=no' to suppress this auto-create (this is  General parameter).

 

Note: to prevent accidentally creating a huge file in the /dev/rdsk/ directory because an incorrectly entered 50+ digit random hexadecimal lun name, Vdbench will refuse to create new file names that start with /dev/. Bad things happen when your root directory fills up J.

 

You can also use size= to give Vdbench access to only a portion of your volume, though for that you can also use 'range=' below.

 

#### **1.13.6** ***\*'range=(min,max)': Limit Seek\**** ***\*Range\****

 

By default, the whole SD will be used. To limit the seek range for a workload, specify the starting and ending range of the SD: 'range=(40,60)' will limit I/O activity starting at 40% into the SD and ending at 60% into the SD.

If the max value is larger than 100 but smaller than 200, Vdbench will consider this a wrap across the end of your volume. For instance with range=(90,110) , Vdbench will generate an I/O workload using the last 10% and the first 10% of your volume.

 

When the values are greater than 200, the values will be considered given in *bytes* instead of in

*percentages*; e.g., ‘range=(1g,2g)’.

 

Note: the 'range=' parameter may not be used during [Sd concatenation](#_bookmark135). Note: hitarea if needed will be at the beginning of the range.



 

#### **1.13.7** ***\*'threads=nn': Maximum Number of\**** ***\*Concurrent outstanding\**** ***\*I/Os\****

 

'threads=nn' specifies the maximum number of concurrent I/O that can be outstanding for this SD. Be aware that depending on the storage subsystem, some of these I/Os may be queued inside of the operating system (wait queue). (On Solaris, check file kstat.html).

 

Warning: If you specify a LUN that physically consists of multiple disk drives, the thread count determines the maximum concurrency for the LUN, not for the disks. A total of 8 concurrent threads for a total of 16 physical disks will not allow for much concurrency. Also, for Solaris, make sure that your *sd_max_throttle* parameter in */etc/system* allows the requested amount of concurrency.

 

Note: be aware that the maximum concurrency will only occur when there is enough demand. Requesting 10 iops against a device that can handle 1000 iops will not give you the concurrency that you request with the thread= parameter.

 

Note: This SD parameter will be ignored when using [SD concatenation](#_bookmark135).

 

This parameter may be overridden using the ['forthreads=' ](#_bookmark126)parameter.

 

 

#### **1.13.8** ***\*'hitarea=nn': Storage Size for Cache\**** ***\*Hits\****

 

See [read hit percentage ](#_bookmark101)for an explanation. Default value is 1MB.

 

 

#### **1.13.9** ***\*'journal=name': Directory Name for Journal\**** ***\*File\****

 

Used with journaling only. Journal files are needed for each Storage Definition and are created by default in the current directory. The file names are 'sdname.jnl' and 'sdname.map', where 'sdname' is the name of the SD. The .jnl file is the actual journal file, while the .map file is a backup of the copy of the latest Data Validation map residing in the .jnl file.

See [Data Validation and Journaling ](#_bookmark148)for a more detailed description of data validation and journaling.

 

When as part of the Data Validation and journaling testing you bring down your OS it is imperative that all writes to the journal file are synchronous. If your OS or file system does not handle this properly you could end up with a corrupted journal file. A corrupted journal file means that the results will be unpredictable during journal recovery.

 

Journaling therefore allows you to specify a RAW device, e.g. journal=/dev/xxxx, bypassing the possibly faulty file system code.



 

#### **1.13.10** ***\*‘offset=’: Don’t start at byte zero of a\**** ***\*LUN\****

 

Vdbench always starts at the beginning of a LUN, but some times it is needed to modify that. Some times a LUN does not start at an exact required physical boundary and this parameter allows you do adjust for that. The offset is in bytes and must be a multiple of 512.

 

Note: Vdbench never accesses block zero on any raw volume. This has been done to make sure that it never overwrites a volume label and/or vtoc.

 

 

 

#### **1.13.11** ***\*‘align=’: Determine lba boundary for random\**** ***\*seeks.\****

 

Whenever Vdbench generates an random lba (logical byte address) it by default is always on a block boundary (xfersize=). Use the ‘align=’ parameter to change that to always generate an LBA on a different alignment. The align= value is in bytes and must be a multiple of 512.

This parameter may not be used when dedupratio= is specified.



 

 

 

#### **1.13.12** ***\*‘openflags=’: control over open and close of luns or\**** ***\*files\****

 

This parameter allows you to control what parameters are passed to the system’s open and close functions. By default write operations are handled according to how the file system is mounted or for raw devices, how the device normally operates. This can mean that a write operation completes as soon as the data is stored in system cache. This makes for very good performance, but does not really exercise the storage.

 

Openflags can be specified for SD, WD, FSD, FWD, and RD parameters. Options (you can create any combination of these)

| Solaris:     | (xx_SYNC descriptions found in man open)Please believe me, I have never really figured out what these options *mean*. |
| ------------ | ------------------------------------------------------------ |
| o_dsync      | Write I/O operations on the file descriptor complete as defined bysynchronized I/O data integrity completion |
| o_rsync      | Read I/O operations on the file descriptor complete at the same level of integrity as specified by the O_DSYNC and O_SYNC flags.If both O_DSYNC and O_RSYNC are set in oflag, all I/O operations on the file descriptor complete as defined by synchronized I/O data integrity completion. If both O_SYNC and O_RSYNC are set in oflag, all I/O operations on the file descriptor complete as defined by synchronized I/O file integrity completion. |
| o_sync       | Write I/O operations on the file descriptor complete as defined bysynchronized I/O file integrity completion. |
| 0x……         | Any hex value, to be passed to the open() function.          |
| fsync        | Call fsync() before the file is closed.                      |
| directio     | Calls the directio() function after the file is opened, using ‘DIRECTIO_ON’ |
| directio_off | Calls the directio() function after the file is opened, using ‘DIRECTIO_OFF’.This one is meant to be used if a previous failed I/O run left the target file name with directio active and you want to forcibly remove that status. |
| clear_cache  | When using directio() for an NFS mounted file, any data still residing in file system cache will continue to be used, circumventing the directio() request. This option will forcibly clear any existing data from cache using mmap()functions. |

 

 

| Linux:                |                                                              |
| --------------------- | ------------------------------------------------------------ |
| o_direct ordirectio   | Gives you raw access to a full volume (no partition).  This parameter is*required* when using /dev/xxx volumes |
| o_dsynco_rsync o_sync | These three all pass ‘0x01000’ to the Linux open() function.*I highly suggest you check /usr/include/bits/fcntl.h since not all flavors of Linux use the same bits. Then if needed code openflags=0x…. instead.* |
| fsync                 | Call fsync() before the file is closed.                      |



 

| 0x…… | Any hex value, to be passed to the open() function. |
| ---- | --------------------------------------------------- |
|      |                                                     |

 

| Windows:                       |                                                       |
| ------------------------------ | ----------------------------------------------------- |
| directio o_dsync o_rsynco_sync | Opens a file using the ' FILE_FLAG_NO_BUFFERING' flag |

 

| AIX                 |                                                     |
| ------------------- | --------------------------------------------------- |
| o_dsync             | Passes 0x00400000 to open().                        |
| o_rsync             | Passes 0x00200000 to open().                        |
| o_sync              | Passes 0x00000010 to open().                        |
| o_direct ordirectio | Passes 0x08000000 to open().                        |
| 0x……                | Any hex value, to be passed to the open() function. |

 

| MAC OSX:                                                     |                                  |
| ------------------------------------------------------------ | -------------------------------- |
| OSX does not allow flags to be passed to open(), instead, after the open() request Vdbench callsthe fcntl() function. |                                  |
| f_nocache                                                    | Decimal 48 is passed to fcntl()  |
| directio                                                     | Decimal 48 is passed to fcntl()  |
| nnn                                                          | Decimal nnn is passed to fcntl() |



 

 

 

#### **1.13.13** ***\*streams=: Independent sequential\**** ***\*streams.\****

 

Note: In Vdbench 503 this was an SD parameter; it has been moved to WD, and its meaning has slightly changed (the 'stream size' subparameter has been removed).

 

The streams=nn parameter overrides the default processing done when reading or writing sequential data. By default Vdbench just reads or writes the whole lun or file (SD, or concatenated SD), or only a portion of it when the range= parameter is used, but the streams= parameter allows multiple concurrent sequential streams to be active.

 

streams=*stream_count.* This parameter works together with the threads= parameter. The file or lun is split into 'stream_count' pieces.

 

Example: wd=wd1,…..,streams=10

 

Each SD (or the concatenated SD) is split into 10 equally sized smaller pieces. The requested threads are spread out over the streams, with as a possible result that some streams may not get the same amount of threads than other streams. This is unacceptable, so Vdbench will increase the threadcount to make sure each stream gets the same amount of threads.

Any adjustment in the threadcount will be reported to you.

 

Note that, since this is sequential I/O, a stream may run on only ONE JVM, this to avoid for instances reading blocks 1,1,2,2,3,3,4,4, etc.

 

Note: The stream count must be a multiple of the amount of JVMs used, this to make sure that the requested stream count can be equally spread around these JVMs. This also will be adjusted by Vdbench if needed.

See [Multi JVM execution](#_bookmark35).

 

Note: rhpct= and whpct= are ignored when streams are requested. However, if you really want to run cache hits with streams, just code size='small' to force all I/O to cache.

 

Note: Only ONE Workload Definition (WD) may be active when using streams.