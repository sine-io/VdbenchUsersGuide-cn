Data Deduplication is built into Vdbench with the understanding that the dedup logic included in the target storage device looks at each n-byte data block to see if a block with identical content already exists. When there is a match the block no longer needs to be written to storage and a pointer to the already existing block is stored instead.

Since it is possible for dedup and data compression algorithms to be used at the same time, dedup by default generates data patterns that do not compress.

 

| Dedup parameters are usually General Parameters and must be specified at the top of your parameter file BEFORE the first Host Definition (HD) or if HD is not used, before the first SD or FSD. Dedup parameters, if needed, can also be specified as a set of sub parameters for an SD. |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| dedupunit=nn                                                 | The size of a data block that dedup tries to match with already existing data. There is no longer a default of 128k.There may be only ONE dedupunit parameter in a Vdbench execution, and that must be specified as a General Parameter (though the other Dedup parameters may be specified for each SD). |
| dedupratio=n                                                 | Ratio between the original data and the actually written data, e.g. dedupratio=2 for a 2:1 ratio. Default: no dedup, or dedupratio=1 |
| dedupsets=nn                                                 | How many different sets or groups of duplicate blocks to have. See below. Default: dedupsets=5% (You can also just code a numeric value, e.g. dedupsets=100) |
| deduphotsets=(n,n, . . .)                                    | How many hot sets, in pairs, e.g. deduphotsets=(10,2,20,5) .Ten sets of two each, then 20 sets of five each. See below. |
| dedupflipflop=no/yes/hot                                     | Default 'no'. Activate the Dedup flip-flop mechanism either for all duplicate sets or only for hot sets. |

 

 

For a Storage Definition (SD) dedup is controlled on an SD level; For a File System Definition (FSD) dedup is controlled on an FSD level, so not on an individual file level.

 

 

There are two different dedup data patterns that Vdbench creates:



 

#### **1.18.1** ***\*Unique\**** ***\*blocks\****

Unique blocks: These blocks are unique and will always be unique, even when they are rewritten. In other words, a unique block will be rewritten with a different content than all its previous versions.

 

 

#### **1.18.2** ***\*Duplicate\**** ***\*blocks.\****

 

As mentioned above, the unique block’s data contents will change each time it is written. The data pattern includes the current time of day in microseconds, together with the SD or FSD name. This makes the content pretty unique unless of course the same block to the same SD is written more than once within the same microsecond.

 

For the duplicate blocks that course won't work. Initially I had planned to never change these blocks until I realized that if I do not change them there will never be another physical disk write because there always will be an already existing copy of each duplicate block. That makes for great benchmark numbers, but that never is my objective. Honesty is always the only way to go. But changing the contents of these blocks for each write operation then causes a new problem: I won’t get my expected dedupratio. Catch 22. Continued below.

 

 

#### **1.18.3** ***\*Dedup\**** ***\*flipflop\****

 

That’s when I decided that yes, I will change the contents, but only once. And the next time that this block is written it will be changed back to its original content, (flip flop). It does mean that my expected dedupratio will be a little off, this because within a set of duplicates there now can be two different data contents, but it stays close. I typically see 1.87:1 instead of the requested 2:1, which is close enough.

 

If there is a better way, let me know. This is the best that I could come up with at this time. I do understand that once all dedup sets have blocks in both the ‘flip’ and ‘flop’ state all physical write activity will cease as long as there is a minimum of one block in each state. Suggestions for improvement are always welcome.

 

Flipflop in earlier versions of Vdbench was always on, but I had so many confused users that I decided to now only use flipflop when specifically requested. Why were users confused? Not 100% sure but possibly because they started running with dedupratio=nn without reading further. I can't blame them though, I only read 100+ pages when the name Stephen King is on the cover.

J

 

 

#### **1.18.4** ***\*Duplicate blocks and duplicate\**** ***\*'sets'.\****

With dedupratio=1 there of course will not be any duplicate blocks.



 

Duplicate blocks as the name indicates are duplicates of each other. They are not all duplicates of one single block though. That would have been too easy. There are ‘nn’ sets or groups of duplicate blocks. All blocks within a set are duplicates of each other. How many sets are there? I have set the default to dedupsets=5%, or 5% of the estimated total amount of dedupunit=nn blocks.

Example: a 128m SD, for 1024 128k blocks. There will be (5% of 1024) 51 sets of duplicate blocks.

Dedupratio=2 ultimately will result in wanting 512 data blocks to be written to disk and 512 blocks that are duplicates of other blocks.

‘512-51 = 461 unique blocks’ + ‘512+51=563 duplicate blocks’ = 1024 blocks.

The 461 unique blocks and the 51 sets make for a total of 512 different data blocks that are written to disk. 1024 / 512 = 2:1 dedup ratio. (The real numbers will be slightly different because of integer rounding and/or truncation).

The sets described above are by default evenly spread out over all of the used storage.

 

 

Dedupsets have the following contents: 0xkk00ttttnnnnnnnn

kk: Data Validation key when Data Validation is active.

tttt: Dedup type. There is normally only one dedup type 0x01, unless you specify different dedup ratios to specific SDs.

nnnnnnnn: A set number, starting at 0x00000001.

 

#### **1.18.5** ***\*Duplicate 'hot\**** ***\*sets'.\****

 

The deduphotsets= parameter gives the user some control over how the duplicate blocks are spread out over the storage.

In above example there are 563 duplicate blocks spread out over 51 sets, or about 11 blocks per set.

 

The 'deduphotsets=(nn,nn,..,..)' parameter allows you to control how many duplicate blocks some of these sets have, for instance:

 

*deduphotsets=(10,2,20,3,10,20)*

 

Hot sets and its blocks are always allocated at the very beginning of a LUN.

The above parameter will create 10 hot sets each containing only two duplicate blocks, followed by 20 sets with only three blocks, and then 10 sets each containing 20 blocks, for a total of 40 sets and 280 blocks.



 

What can you do with these hot sets? You can for instance decide that as part of the overall dedupratio you want one duplicate set that has so many duplicates that it 'may' cause performance problems, testing the storage device's dedup performance to its limits.

Or, you maybe want to have a duplicate set that has only a few blocks in it. (Only one block in a set makes the block unique and therefore no longer a duplicate).

Why only a few blocks? Let me try to explain, Iwill very likely be using the wrong terminology, but I still I hope to make sense.

This is how I understand Dedup: a hash is created using all the data stored in a data block. That hash will be stored in a hash table. If the hash already exists it means that this block is a duplicate, and instead of storing the data block again, only a reference to the original data block is stored.

Using Vdbench, how likely is it that once a duplicate block has been written all references to that specific duplicate block disappear? Or that any new duplicate hashes are created? This will only happen if ALL these blocks are rewritten with a different value, and of course no new references to this hash are generated.

By default, which means without flipflop, Vdbench will always rewrite the EXACT same data content for each duplicate. With the EXACT data being rewritten, nothing really changes.

Using flipflop however, there always is a small change in the data, so each time one of the duplicate blocks is rewritten there is a chance that either a new hash needs to be created, or of an old hash to disappear. How much chance?

I ran some simulations. Using random writes, and using above's '51 dedup sets' example with a duplicate set containing 11 blocks it is highly unlikely that all blocks in that set are all 'flip' or all 'flop'. In other words: after initial creation, a hash will always stay around, no hashes will disappear, and no new hashes will be created.

With only two, three or four blocks in a duplicate set however there is a very fair chance that all the blocks in that set can change from all 'flip' to all 'flop' and back.

Using small hot sets we can now make sure that there is some variety in what is happening to the dedup hash table.

 

One problem though: if we have a large LUN, and a few small dedup set, and are doing random writes across the whole LUN, it can take quite a while for these hot sets to be referenced often enough. Of course, if you create a very small lun, or a very small file, that is not a worry. But for large luns we'll need a way for the hot sets to be referenced frequently enough.

When using hot sets you therefore should start using ['hot bands'.](#_bookmark135)

When using hot bands the majority of the i/o is done at the beginning of the hotband, doing exactly what we need to increase the amount of i/o to the hot sets which always are at the beginning of a lun.

 

 

#### **1.18.6** ***\*Vdbench xfersize=\**** ***\*limitations.\****

Earlier versions of Dedup in Vdbench had a requirement that all xfersizes used were a multiple of the dedup unit. That no longer is needed.



 

#### **1.18.7** ***\*Use of Data Validation\**** ***\*code.\****

So how to keep track of what the current content is of a duplicate data block?

Data Validation already had everything that is needed; it knows exactly what is written where. Using this ability was a very easy decision to make. Of course, unless specifically requested the actual contents of a block after a read operation will not be validated.

So now, when dedup is used, Data Validation instead of keeping track of 126 different data patterns per block now keeps track of only two different data patterns to support the flip-flop mentioned above.

 

The previous version of Vdbench used a memory mapped file to keep track of this. With the change in Vdbench allowing for 48 bits worth of data blocks this started becoming a little too big. Today this information is maintained in a bit map, a bit map residing in the java heap space. The validate=continue option allowing you to pass the flipflop information from one Vdbench execution to the next execution no longer exists.

 

And that's OK though: we know that with flipflop the accuracy of the ultimate Dedup results was not 100%, whether that would be in one Vdbench execution or in the next. This option therefore really no longer offered any real value.

 

 

#### **1.18.8** ***\*Swat/Vdbench Replay with\**** ***\*dedup.\****

One of the great features when you combine Swat and Vdbench is the fact that you can take any customer I/O trace (Solaris and RedHat/OEL), and replay the exact I/O workload whenever and wherever (any OS) you want.

Of course, the originally traced I/O workload likely does not properly follow the above- mentioned requirements of all data transfer sizes and therefore lba’s being a multiple of the dedupunit= size.

For Replay Vdbench adjusts all data transfer sizes to its nearest multiple of the required size and lba, and then also reports the average difference between the original and modified size.

TBD: is this now still needed? Probably not because I now allow any xfersize with Dedup.

 

 

#### **1.18.9** ***\*offset= and align= parameter and\**** ***\*dedup.\****

Because of the requirement for everything to be properly aligned the offset= and align= parameters of course cannot be used with dedup.

TBD: is this now still needed? Probably not because I now allow any xfersize with Dedup.